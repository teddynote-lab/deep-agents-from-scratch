{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc9f89a",
   "metadata": {},
   "source": [
    "# Deep Agent for Research: LangChain ê¸°ë°˜ AI ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ êµ¬ì¶• íŠœí† ë¦¬ì–¼\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” LangChain ë° LangGraphë¥¼ í™œìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ AI ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸(Deep Agent)ë¥¼ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³ , ìš”ì•½ ì •ë³´ë¥¼ ì—ì´ì „íŠ¸ì—ê²Œ ì „ë‹¬í•˜ëŠ” êµ¬ì¡°ë¥¼ í†µí•´ í† í° íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤. ë˜í•œ, ì „ëµì  ì‚¬ê³  ë„êµ¬ì™€ ì„œë¸Œì—ì´ì „íŠ¸(Research Sub-Agent) ìœ„ì„ íŒ¨í„´ì„ ì ìš©í•˜ì—¬ ë³µì¡í•œ ë¦¬ì„œì¹˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤. ì‹¤ìŠµì„ í†µí•´ ì§ì ‘ Deep Agentë¥¼ êµ¬í˜„í•˜ê³ , `deepagents` íŒ¨í‚¤ì§€ë¥¼ í™œìš©í•œ ê°„í¸í•œ ì¶”ìƒí™” ë°©ë²•ë„ ì†Œê°œí•©ë‹ˆë‹¤.\n",
    "\n",
    "**ğŸ† ì£¼ë¡œ ë‹¤ë£¨ëŠ” ë‚´ìš©**\n",
    "1. Deep Agent ì•„í‚¤í…ì²˜ ê°œìš”  \n",
    "   - ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³ , ìµœì†Œí•œì˜ ìš”ì•½ ì •ë³´ë§Œ ì—ì´ì „íŠ¸ì—ê²Œ ì „ë‹¬í•˜ëŠ” êµ¬ì¡° ë° í† í° íš¨ìœ¨ì„± ë¬¸ì œ í•´ê²° ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
    "2. Tavily ê¸°ë°˜ ì›¹ ê²€ìƒ‰ ë„êµ¬(tavily_search) êµ¬í˜„  \n",
    "   - Tavily APIë¥¼ í™œìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ , ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ë©°, ìš”ì•½ ì •ë³´ë¥¼ ìƒì„±í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜ ë° ë„êµ¬ë¥¼ ì§ì ‘ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "3. ì›¹í˜ì´ì§€ ë‚´ìš© ìš”ì•½ ë° íŒŒì¼ ì €ì¥  \n",
    "   - GPT-4o-mini ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì›¹í˜ì´ì§€ ë‚´ìš©ì„ êµ¬ì¡°ì ìœ¼ë¡œ ìš”ì•½í•˜ê³ , íŒŒì¼ëª…ê³¼ ì£¼ìš” í•™ìŠµ ë‚´ìš©ì„ ìë™ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n",
    "4. ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ë° ì—ëŸ¬ í•¸ë“¤ë§  \n",
    "   - HTTP ìš”ì²­ì„ í†µí•œ ì›¹í˜ì´ì§€ ë‚´ìš© ìˆ˜ì§‘, HTMLì„ Markdownìœ¼ë¡œ ë³€í™˜, ì—ëŸ¬ ë°œìƒ ì‹œ ëŒ€ì²´ ìš”ì•½ ì œê³µ ë“± ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
    "5. ì „ëµì  ì‚¬ê³  ë„êµ¬(think_tool) ì„¤ê³„  \n",
    "   - ë¦¬ì„œì¹˜ ì§„í–‰ ìƒí™©ì„ ë¶„ì„í•˜ê³ , ì •ë³´ì˜ ë¶€ì¡±/ì¶©ë¶„ ì—¬ë¶€ë¥¼ í‰ê°€í•˜ë©°, ë‹¤ìŒ ë‹¨ê³„ ê²°ì •ì„ ìœ„í•œ ì „ëµì  ì‚¬ê³  ë„êµ¬ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "6. Deep Agent ë° Research Sub-Agent êµ¬ì„±  \n",
    "   - ë©”ì¸ ì—ì´ì „íŠ¸ì™€ ì„œë¸Œì—ì´ì „íŠ¸ì— ê°ê° ì í•©í•œ ë„êµ¬ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ í• ë‹¹í•˜ì—¬, ì—­í•  ë¶„ë¦¬ì™€ ìœ„ì„ íŒ¨í„´ì„ ì ìš©í•˜ëŠ” ë°©ë²•ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤.\n",
    "7. Deep Agent ì‹¤ìŠµ ë° ì‘ë‹µ ì˜ˆì‹œ  \n",
    "   - ì‹¤ì œë¡œ ì—ì´ì „íŠ¸ì—ê²Œ ë¦¬ì„œì¹˜ ìš”ì²­ì„ ë³´ë‚´ê³ , ì‘ë‹µ ë©”ì‹œì§€ë¥¼ í¬ë§·íŒ…í•˜ì—¬ ê²°ê³¼ë¥¼ í™•ì¸í•˜ëŠ” ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "8. deepagents íŒ¨í‚¤ì§€ í™œìš©  \n",
    "   - ì§ì ‘ êµ¬í˜„í•œ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ, `deepagents` íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ Deep Agentë¥¼ ê°„í¸í•˜ê²Œ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤.\n",
    "\n",
    "**âœ… ì£¼ìš” í‚¤ì›Œë“œ**\n",
    "- Deep Agent\n",
    "- LangChain\n",
    "- LangGraph\n",
    "- Tavily API\n",
    "- Research Sub-Agent\n",
    "- File System Tools\n",
    "- Summarization Model (GPT-4o-mini)\n",
    "- think_tool\n",
    "- Context Offloading\n",
    "- Task Delegation\n",
    "\n",
    "**â­ï¸ ì‚¬ì „ ì§€ì‹**\n",
    "- Python í”„ë¡œê·¸ë˜ë° ê¸°ë³¸ ì§€ì‹\n",
    "- Jupyter Notebook ì‚¬ìš© ê²½í—˜\n",
    "- LangChain ë° LangGraph ê¸°ë³¸ ê°œë… ì´í•´\n",
    "- API í˜¸ì¶œ ë° HTTP ìš”ì²­ ì²˜ë¦¬ ê²½í—˜\n",
    "- ì¸ê³µì§€ëŠ¥ ëª¨ë¸(íŠ¹íˆ LLM) í™œìš© ê²½í—˜ì´ ìˆìœ¼ë©´ í•™ìŠµì— ë„ì›€ì´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5213128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv(override=True)\n",
    "# ì¶”ì ì„ ìœ„í•œ í”„ë¡œì íŠ¸ ì´ë¦„ ì„¤ì •\n",
    "logging.langsmith(\"Deep-Agent-Tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a37e04",
   "metadata": {},
   "source": [
    "## Deep Agent for Research\n",
    "\n",
    "### ê°œìš”\n",
    "\n",
    "![agent_header](./assets/agent_header.png)\n",
    "\n",
    "ì§€ê¸ˆê¹Œì§€ í•™ìŠµí•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ì˜ í•µì‹¬ êµ¬ì¡°ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "- **TODO ë¦¬ìŠ¤íŠ¸**ë¥¼ í™œìš©í•˜ì—¬ ê° ì‘ì—…ì˜ ì§„í–‰ ìƒí™©ì„ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
    "- **íŒŒì¼ ì‹œìŠ¤í…œ**ì„ ì´ìš©í•´ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼(ì›ë³¸ ë°ì´í„°)ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "- **ì„œë¸Œì—ì´ì „íŠ¸**ì—ê²Œ ë¦¬ì„œì¹˜ ì‘ì—…ì„ ìœ„ì„í•˜ì—¬, ê° ì—ì´ì „íŠ¸ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### Search Tool ì„¤ê³„\n",
    "\n",
    "ê²€ìƒ‰ ë„êµ¬ëŠ” ì›¹ì—ì„œ ìˆ˜ì§‘í•œ ì›ë³¸ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³ , ì—ì´ì „íŠ¸ì—ê²ŒëŠ” ìš”ì•½ ì •ë³´ë§Œ ì „ë‹¬í•˜ëŠ” êµ¬ì¡°ë¡œ ì„¤ê³„í•©ë‹ˆë‹¤. ì´ ë°©ì‹ì€ ì¥ê¸°ì ì¸ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ì—ì„œ ë§¤ìš° íš¨ê³¼ì ì´ë©°, ì‹¤ì œ ì‚¬ë¡€ë¡œëŠ” [Manusì˜ Context Engineering](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "#### í•µì‹¬ êµ¬ì„± ìš”ì†Œ\n",
    "\n",
    "1. **ê²€ìƒ‰ ì‹¤í–‰ (`run_tavily_search`)**  \n",
    "   Tavily APIë¥¼ í™œìš©í•˜ì—¬ ì‹¤ì œ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ê²°ê³¼ ê°œìˆ˜ ë° ì£¼ì œ í•„í„°ë§ ë“± ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "2. **ì½˜í…ì¸  ìš”ì•½ (`summarize_webpage_content`)**  \n",
    "   `GPT-4o-mini` ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì›¹í˜ì´ì§€ì˜ ì£¼ìš” ë‚´ìš©ì„ êµ¬ì¡°ì ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤. ìš”ì•½ ê²°ê³¼ì—ëŠ” íŒŒì¼ëª…ê³¼ í•µì‹¬ í•™ìŠµ ë‚´ìš©ì´ í¬í•¨ë©ë‹ˆë‹¤.\n",
    "\n",
    "3. **ê²°ê³¼ ì²˜ë¦¬ (`process_search_results`)**  \n",
    "   HTTPë¥¼ í†µí•´ ì›¹í˜ì´ì§€ ì „ì²´ ë‚´ìš©ì„ ìˆ˜ì§‘í•˜ê³ , `markdownify`ë¥¼ ì´ìš©í•´ HTMLì„ Markdownìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ê° ê²°ê³¼ì— ëŒ€í•´ ìš”ì•½ ì •ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "4. **ì»¨í…ìŠ¤íŠ¸ ì˜¤í”„ë¡œë”© (`tavily_search` tool)**  \n",
    "   ê²€ìƒ‰ ë° ê²°ê³¼ ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•˜ê³ , ì „ì²´ ì›ë³¸ ë°ì´í„°ëŠ” íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. ì—ì´ì „íŠ¸ì—ê²ŒëŠ” ìµœì†Œí•œì˜ ìš”ì•½ ì •ë³´ë§Œ ë°˜í™˜í•˜ì—¬ ë¶ˆí•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ í™•ì¥ì„ ë°©ì§€í•©ë‹ˆë‹¤. ë˜í•œ, LangGraphì˜ `Command`ë¥¼ í™œìš©í•´ íŒŒì¼ê³¼ ë©”ì‹œì§€ë¥¼ ë™ì‹œì— ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "5. **ì „ëµì  ì‚¬ê³  ë„êµ¬ (`think_tool`)**  \n",
    "   ì—ì´ì „íŠ¸ê°€ ë¦¬ì„œì¹˜ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³ , ì •ë³´ì˜ ë¶€ì¡±/ì¶©ë¶„ ì—¬ë¶€ë¥¼ í‰ê°€í•˜ë©°, ë‹¤ìŒ ë‹¨ê³„ì˜ ê³„íšì„ ì„¸ìš¸ ìˆ˜ ìˆë„ë¡ êµ¬ì¡°í™”ëœ ì‚¬ê³  í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ëŸ¬í•œ ì•„í‚¤í…ì²˜ëŠ” ìƒì„¸í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ë¶„ë¦¬ ì €ì¥í•¨ìœ¼ë¡œì¨, ì—ì´ì „íŠ¸ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìµœì†Œí™”í•˜ê³  í† í° íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab75a41",
   "metadata": {},
   "source": [
    "> **ì°¸ê³ **  \n",
    "`create_react_agent` í•¨ìˆ˜ëŠ” LangGraph ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì´ë™ë˜ì—ˆìœ¼ë©°, 1.0 ë²„ì „ ì½”ë“œ ë¦´ë¦¬ìŠ¤ ì´í›„ì—ëŠ” `create_agent`ë¡œ ì´ë¦„ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì— ë”°ë¼ import ë° ì½”ë“œ ì‚¬ìš© ë°©ì‹ì— ì•½ê°„ì˜ ë³€ê²½ì´ ìˆìœ¼ë‹ˆ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ì˜ìƒì—ì„œëŠ” ì´ì „ ì„¤ì •ì´ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìµœì‹  ì½”ë“œ ì˜ˆì œì—ì„œëŠ” ë³€ê²½ëœ ì‚¬í•­ì„ ë°˜ì˜í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d9b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/deep_agents_from_scratch/research_tools.py\n",
    "\"\"\"Research Tools.\n",
    "\n",
    "This module provides search and content processing utilities for the research agent,\n",
    "including web search capabilities and content summarization tools.\n",
    "\"\"\"\n",
    "import os\n",
    "from datetime import datetime\n",
    "import uuid, base64\n",
    "\n",
    "import httpx\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langchain_core.tools import InjectedToolArg, InjectedToolCallId, tool\n",
    "from langgraph.prebuilt import InjectedState\n",
    "from langgraph.types import Command\n",
    "from markdownify import markdownify\n",
    "from pydantic import BaseModel, Field\n",
    "from tavily import TavilyClient\n",
    "from typing_extensions import Annotated, Literal\n",
    "\n",
    "from deep_agents_from_scratch.prompts import SUMMARIZE_WEB_SEARCH\n",
    "from deep_agents_from_scratch.state import DeepAgentState\n",
    "\n",
    "# ìš”ì•½ ëª¨ë¸ ì´ˆê¸°í™” (GPT-4o-mini) ë° Tavily API í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "summarization_model = init_chat_model(model=\"openai:gpt-4o-mini\")\n",
    "tavily_client = TavilyClient()\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    \"\"\"Schema for webpage content summarization.\"\"\"\n",
    "    # ìš”ì•½ ê²°ê³¼ íŒŒì¼ëª… ë° ì£¼ìš” í•™ìŠµ ë‚´ìš© ì €ì¥ì„ ìœ„í•œ í•„ë“œ ì •ì˜\n",
    "    filename: str = Field(description=\"Name of the file to store.\")\n",
    "    summary: str = Field(description=\"Key learnings from the webpage.\")\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ë¬¸ìì—´ë¡œ ë°˜í™˜\n",
    "def get_today_str() -> str:\n",
    "    \"\"\"Get current date in a human-readable format.\"\"\"\n",
    "    return datetime.now().strftime(\"%a %b %-d, %Y\")\n",
    "\n",
    "# Tavily APIë¥¼ í™œìš©í•œ ë‹¨ì¼ ì¿¼ë¦¬ ì›¹ ê²€ìƒ‰ ì‹¤í–‰\n",
    "def run_tavily_search(\n",
    "    search_query: str,\n",
    "    max_results: int = 1,\n",
    "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n",
    "    include_raw_content: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"Perform search using Tavily API for a single query.\n",
    "\n",
    "    Args:\n",
    "        search_query: Search query to execute\n",
    "        max_results: Maximum number of results per query\n",
    "        topic: Topic filter for search results\n",
    "        include_raw_content: Whether to include raw webpage content\n",
    "\n",
    "    Returns:\n",
    "        Search results dictionary\n",
    "    \"\"\"\n",
    "    result = tavily_client.search(\n",
    "        search_query,\n",
    "        max_results=max_results,\n",
    "        include_raw_content=include_raw_content,\n",
    "        topic=topic\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "# ì›¹í˜ì´ì§€ ì›ë³¸ ë‚´ìš©ì„ ìš”ì•½ ëª¨ë¸ë¡œ êµ¬ì¡°ì  ìš”ì•½ ë° íŒŒì¼ëª… ìƒì„±\n",
    "def summarize_webpage_content(webpage_content: str) -> Summary:\n",
    "    \"\"\"Summarize webpage content using the configured summarization model.\n",
    "\n",
    "    Args:\n",
    "        webpage_content: Raw webpage content to summarize\n",
    "\n",
    "    Returns:\n",
    "        Summary object with filename and summary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Structured output ëª¨ë¸ì„ í™œìš©í•œ ìš”ì•½ ê²°ê³¼ ìƒì„±\n",
    "        structured_model = summarization_model.with_structured_output(Summary)\n",
    "\n",
    "        # HumanMessage í”„ë¡¬í”„íŠ¸ë¡œ ìš”ì•½ ë° íŒŒì¼ëª… ë™ì‹œ ìƒì„±\n",
    "        summary_and_filename = structured_model.invoke([\n",
    "            HumanMessage(content=SUMMARIZE_WEB_SEARCH.format(\n",
    "                webpage_content=webpage_content,\n",
    "                date=get_today_str()\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "        return summary_and_filename\n",
    "\n",
    "    except Exception:\n",
    "        # ìš”ì•½ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ íŒŒì¼ëª… ë° ì•ë¶€ë¶„ ìš”ì•½ ë°˜í™˜\n",
    "        return Summary(\n",
    "            filename=\"search_result.md\",\n",
    "            summary=webpage_content[:1000] + \"...\" if len(webpage_content) > 1000 else webpage_content\n",
    "        )\n",
    "\n",
    "# Tavily ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°›ì•„ ê° ê²°ê³¼ì— ëŒ€í•´ ìš”ì•½ ë° íŒŒì¼ëª… ìƒì„±, ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨\n",
    "def process_search_results(results: dict) -> list[dict]:\n",
    "    \"\"\"Process search results by summarizing content where available.\n",
    "\n",
    "    Args:\n",
    "        results: Tavily search results dictionary\n",
    "\n",
    "    Returns:\n",
    "        List of processed results with summaries\n",
    "    \"\"\"\n",
    "    processed_results = []\n",
    "\n",
    "    # HTTP ìš”ì²­ìš© í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° 30ì´ˆ íƒ€ì„ì•„ì›ƒ ì„¤ì •\n",
    "    HTTPX_CLIENT = httpx.Client(timeout=30.0)  # Add 30 second timeout\n",
    "\n",
    "    for result in results.get('results', []):\n",
    "\n",
    "        # ê²°ê³¼ URL ì¶”ì¶œ\n",
    "        url = result['url']\n",
    "\n",
    "        # URL ì ‘ì† ë° ì—ëŸ¬ í•¸ë“¤ë§, HTMLì„ Markdownìœ¼ë¡œ ë³€í™˜\n",
    "        try:\n",
    "            response = HTTPX_CLIENT.get(url)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                # HTML ì›ë¬¸ì„ Markdownìœ¼ë¡œ ë³€í™˜ í›„ ìš”ì•½\n",
    "                raw_content = markdownify(response.text)\n",
    "                summary_obj = summarize_webpage_content(raw_content)\n",
    "            else:\n",
    "                # Tavilyê°€ ì œê³µí•œ ìš”ì•½ ë° ì›ë³¸ ë‚´ìš© ì‚¬ìš©\n",
    "                raw_content = result.get('raw_content', '')\n",
    "                summary_obj = Summary(\n",
    "                    filename=\"URL_error.md\",\n",
    "                    summary=result.get('content', 'Error reading URL; try another search.')\n",
    "                )\n",
    "        except (httpx.TimeoutException, httpx.RequestError) as e:\n",
    "            # íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ì—°ê²° ì˜¤ë¥˜ ë°œìƒ ì‹œ ëŒ€ì²´ ìš”ì•½ ë° íŒŒì¼ëª… ì§€ì •\n",
    "            raw_content = result.get('raw_content', '')\n",
    "            summary_obj = Summary(\n",
    "                filename=\"connection_error.md\",\n",
    "                summary=result.get('content', f'Could not fetch URL (timeout/connection error). Try another search.')\n",
    "            )\n",
    "\n",
    "        # íŒŒì¼ëª… ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ UUID ê¸°ë°˜ ê³ ìœ ê°’ ìƒì„± ë° íŒŒì¼ëª…ì— ì¶”ê°€\n",
    "        uid = base64.urlsafe_b64encode(uuid.uuid4().bytes).rstrip(b\"=\").decode(\"ascii\")[:8]\n",
    "        name, ext = os.path.splitext(summary_obj.filename)\n",
    "        summary_obj.filename = f\"{name}_{uid}{ext}\"\n",
    "\n",
    "        # ìµœì¢… ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ìš”ì•½, íŒŒì¼ëª…, ì›ë³¸ ë‚´ìš© ë“± ì €ì¥\n",
    "        processed_results.append({\n",
    "            'url': result['url'],\n",
    "            'title': result['title'],\n",
    "            'summary': summary_obj.summary,\n",
    "            'filename': summary_obj.filename,\n",
    "            'raw_content': raw_content,\n",
    "        })\n",
    "\n",
    "    return processed_results\n",
    "\n",
    "# Tavily ê²€ìƒ‰ ë° ê²°ê³¼ íŒŒì¼ ì €ì¥, ì—ì´ì „íŠ¸ì—ê²Œ ìµœì†Œ ìš”ì•½ ì •ë³´ë§Œ ë°˜í™˜í•˜ëŠ” ë„êµ¬ í•¨ìˆ˜\n",
    "@tool(parse_docstring=True)\n",
    "def tavily_search(\n",
    "    query: str,\n",
    "    state: Annotated[DeepAgentState, InjectedState],\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId],\n",
    "    max_results: Annotated[int, InjectedToolArg] = 1,\n",
    "    topic: Annotated[Literal[\"general\", \"news\", \"finance\"], InjectedToolArg] = \"general\",\n",
    ") -> Command:\n",
    "    \"\"\"Search web and save detailed results to files while returning minimal context.\n",
    "\n",
    "    Performs web search and saves full content to files for context offloading.\n",
    "    Returns only essential information to help the agent decide on next steps.\n",
    "\n",
    "    Args:\n",
    "        query: Search query to execute\n",
    "        state: Injected agent state for file storage\n",
    "        tool_call_id: Injected tool call identifier\n",
    "        max_results: Maximum number of results to return (default: 1)\n",
    "        topic: Topic filter - 'general', 'news', or 'finance' (default: 'general')\n",
    "\n",
    "    Returns:\n",
    "        Command that saves full results to files and provides minimal summary\n",
    "    \"\"\"\n",
    "    # Tavily ê²€ìƒ‰ ì‹¤í–‰ ë° ì›ë³¸ í¬í•¨ ê²°ê³¼ ë°˜í™˜\n",
    "    search_results = run_tavily_search(\n",
    "        query,\n",
    "        max_results=max_results,\n",
    "        topic=topic,\n",
    "        include_raw_content=True,\n",
    "    )\n",
    "\n",
    "    # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ë° íŒŒì¼ëª… ìƒì„±\n",
    "    processed_results = process_search_results(search_results)\n",
    "\n",
    "    # íŒŒì¼ ì‹œìŠ¤í…œì— ê²°ê³¼ ì €ì¥ ë° ìš”ì•½ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "    files = state.get(\"files\", {})\n",
    "    saved_files = []\n",
    "    summaries = []\n",
    "\n",
    "    for i, result in enumerate(processed_results):\n",
    "        # ìš”ì•½ ëª¨ë¸ì—ì„œ ìƒì„±ëœ íŒŒì¼ëª… ì‚¬ìš©\n",
    "        filename = result['filename']\n",
    "\n",
    "        # íŒŒì¼ ë‚´ìš©: ì œëª©, URL, ì¿¼ë¦¬, ë‚ ì§œ, ìš”ì•½, ì›ë³¸ ë‚´ìš© í¬í•¨\n",
    "        file_content = f\"\"\"# Search Result: {result['title']}\n",
    "\n",
    "**URL:** {result['url']}\n",
    "**Query:** {query}\n",
    "**Date:** {get_today_str()}\n",
    "\n",
    "## Summary\n",
    "{result['summary']}\n",
    "\n",
    "## Raw Content\n",
    "{result['raw_content'] if result['raw_content'] else 'No raw content available'}\n",
    "\"\"\"\n",
    "\n",
    "        files[filename] = file_content\n",
    "        saved_files.append(filename)\n",
    "        summaries.append(f\"- {filename}: {result['summary']}...\")\n",
    "\n",
    "    # ì—ì´ì „íŠ¸ì—ê²Œ ë°˜í™˜í•  ìµœì†Œ ìš”ì•½ ë©”ì‹œì§€ ìƒì„± (íŒŒì¼ëª… ë° ìš”ì•½ ë¦¬ìŠ¤íŠ¸)\n",
    "    summary_text = f\"\"\"ğŸ” Found {len(processed_results)} result(s) for '{query}':\n",
    "\n",
    "{chr(10).join(summaries)}\n",
    "\n",
    "Files: {', '.join(saved_files)}\n",
    "ğŸ’¡ Use read_file() to access full details when needed.\"\"\"\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"files\": files,\n",
    "            \"messages\": [\n",
    "                ToolMessage(summary_text, tool_call_id=tool_call_id)\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# ë¦¬ì„œì¹˜ ì§„í–‰ ìƒí™© ë¶„ì„ ë° ì „ëµì  íŒë‹¨ì„ ìœ„í•œ ì‚¬ê³  ë„êµ¬ í•¨ìˆ˜\n",
    "@tool(parse_docstring=True)\n",
    "def think_tool(reflection: str) -> str:\n",
    "    \"\"\"Tool for strategic reflection on research progress and decision-making.\n",
    "\n",
    "    Use this tool after each search to analyze results and plan next steps systematically.\n",
    "    This creates a deliberate pause in the research workflow for quality decision-making.\n",
    "\n",
    "    When to use:\n",
    "    - After receiving search results: What key information did I find?\n",
    "    - Before deciding next steps: Do I have enough to answer comprehensively?\n",
    "    - When assessing research gaps: What specific information am I still missing?\n",
    "    - Before concluding research: Can I provide a complete answer now?\n",
    "    - How complex is the question: Have I reached the number of search limits?\n",
    "\n",
    "    Reflection should address:\n",
    "    1. Analysis of current findings - What concrete information have I gathered?\n",
    "    2. Gap assessment - What crucial information is still missing?\n",
    "    3. Quality evaluation - Do I have sufficient evidence/examples for a good answer?\n",
    "    4. Strategic decision - Should I continue searching or provide my answer?\n",
    "\n",
    "    Args:\n",
    "        reflection: Your detailed reflection on research progress, findings, gaps, and next steps\n",
    "\n",
    "    Returns:\n",
    "        Confirmation that reflection was recorded for decision-making\n",
    "    \"\"\"\n",
    "    return f\"Reflection recorded: {reflection}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d167c32",
   "metadata": {},
   "source": [
    "## Deep Agent\n",
    "\n",
    "ì§€ê¸ˆê¹Œì§€ í•™ìŠµí•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ì‹¤ì œ ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ì˜ ì „ì²´ êµ¬ì¡°ë¥¼ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- `think_tool`ê³¼ ì•ì„œ êµ¬í˜„í•œ `search_tool`ì„ ë¦¬ì„œì¹˜ ì„œë¸Œì—ì´ì „íŠ¸ì—ê²Œ í• ë‹¹í•˜ì—¬, ì •ë³´ íƒìƒ‰ê³¼ ì „ëµì  ì‚¬ê³ ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "- ìƒìœ„ ì—ì´ì „íŠ¸(Parent Agent)ì—ëŠ” íŒŒì¼ ì‹œìŠ¤í…œ ë„êµ¬, `think_tool`, ê·¸ë¦¬ê³  `task` ë„êµ¬ë¥¼ í•¨ê»˜ ì œê³µí•˜ì—¬, ì—­í•  ë¶„ë¦¬ì™€ íš¨ìœ¨ì ì¸ ì‘ì—… ìœ„ì„ì´ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì™€ ê°™ì€ êµ¬ì¡°ë¥¼ í†µí•´ ê° ì—ì´ì „íŠ¸ê°€ ìì‹ ì˜ ì—­í• ì— ì§‘ì¤‘í•  ìˆ˜ ìˆìœ¼ë©°, ë³µì¡í•œ ë¦¬ì„œì¹˜ ì›Œí¬í”Œë¡œìš°ë„ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df972281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# from langgraph.prebuilt import create_react_agent\n",
    "from langchain.agents import create_agent\n",
    "from utils import show_prompt, stream_agent\n",
    "\n",
    "from deep_agents_from_scratch.file_tools import ls, read_file, write_file\n",
    "from deep_agents_from_scratch.prompts import (\n",
    "    FILE_USAGE_INSTRUCTIONS,\n",
    "    RESEARCHER_INSTRUCTIONS,\n",
    "    SUBAGENT_USAGE_INSTRUCTIONS,\n",
    "    TODO_USAGE_INSTRUCTIONS,\n",
    ")\n",
    "from deep_agents_from_scratch.research_tools import (\n",
    "    tavily_search,\n",
    "    think_tool,\n",
    "    get_today_str,\n",
    ")\n",
    "from deep_agents_from_scratch.state import DeepAgentState\n",
    "from deep_agents_from_scratch.task_tool import _create_task_tool\n",
    "from deep_agents_from_scratch.todo_tools import write_todos, read_todos\n",
    "\n",
    "# ê¸°ë³¸ LLM ëª¨ë¸ ì´ˆê¸°í™” ë° ì˜¨ë„ ì„¤ì • (Anthropic Claude Sonnet 4 ì‚¬ìš©)\n",
    "model = init_chat_model(model=\"anthropic:claude-sonnet-4-20250514\", temperature=0.0)\n",
    "\n",
    "# ë¦¬ì„œì¹˜ ì„œë¸Œì—ì´ì „íŠ¸ ë™ì‹œ ì‹¤í–‰ ìµœëŒ€ ê°œìˆ˜ ë° ë°˜ë³µ íšŸìˆ˜ ì œí•œ ì„¤ì •\n",
    "max_concurrent_research_units = 3\n",
    "max_researcher_iterations = 3\n",
    "\n",
    "# ë¦¬ì„œì¹˜ ì„œë¸Œì—ì´ì „íŠ¸ì— í• ë‹¹í•  ë„êµ¬ ë¦¬ìŠ¤íŠ¸ (ì›¹ ê²€ìƒ‰ ë° ì „ëµì  ì‚¬ê³  ë„êµ¬)\n",
    "sub_agent_tools = [tavily_search, think_tool]\n",
    "# ë©”ì¸ ì—ì´ì „íŠ¸ì— í• ë‹¹í•  íŒŒì¼ ì‹œìŠ¤í…œ ë° TODO ê´€ë¦¬ ë„êµ¬, ì „ëµì  ì‚¬ê³  ë„êµ¬ í¬í•¨\n",
    "built_in_tools = [ls, read_file, write_file, write_todos, read_todos, think_tool]\n",
    "\n",
    "# ë¦¬ì„œì¹˜ ì„œë¸Œì—ì´ì „íŠ¸ êµ¬ì„± ì •ë³´ (ì´ë¦„, ì„¤ëª…, í”„ë¡¬í”„íŠ¸, ë„êµ¬ ëª©ë¡)\n",
    "research_sub_agent = {\n",
    "    \"name\": \"research-agent\",\n",
    "    \"description\": \"Delegate research to the sub-agent researcher. Only give this researcher one topic at a time.\",\n",
    "    \"prompt\": RESEARCHER_INSTRUCTIONS.format(date=get_today_str()),\n",
    "    \"tools\": [\"tavily_search\", \"think_tool\"],\n",
    "}\n",
    "\n",
    "# ì„œë¸Œì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—… ìœ„ì„ì„ ìœ„í•œ Task Tool ìƒì„± (ëª¨ë¸, ìƒíƒœ, ë„êµ¬ ì „ë‹¬)\n",
    "task_tool = _create_task_tool(\n",
    "    sub_agent_tools, [research_sub_agent], model, DeepAgentState\n",
    ")\n",
    "\n",
    "# ë©”ì¸ ì—ì´ì „íŠ¸ì— ìœ„ì„ ë„êµ¬ í¬í•¨ ì „ì²´ ë„êµ¬ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ê²€ìƒ‰ ë„êµ¬ ì§ì ‘ ì‚¬ìš© ê°€ëŠ¥)\n",
    "delegation_tools = [task_tool]\n",
    "all_tools = (\n",
    "    sub_agent_tools + built_in_tools + delegation_tools\n",
    ")  # search available to main agent for trivial cases\n",
    "\n",
    "# ì„œë¸Œì—ì´ì „íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„± (ìµœëŒ€ ë™ì‹œ ë¦¬ì„œì¹˜ ê°œìˆ˜, ë°˜ë³µ íšŸìˆ˜, ë‚ ì§œ í¬í•¨)\n",
    "SUBAGENT_INSTRUCTIONS = SUBAGENT_USAGE_INSTRUCTIONS.format(\n",
    "    max_concurrent_research_units=max_concurrent_research_units,\n",
    "    max_researcher_iterations=max_researcher_iterations,\n",
    "    date=datetime.now().strftime(\"%a %b %-d, %Y\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbcadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¦¬ì„œì²˜ ì—ì´ì „íŠ¸ í”„ë¡¬í”„íŠ¸ ì¶œë ¥\n",
    "show_prompt(RESEARCHER_INSTRUCTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f88991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ê´€ë¦¬, íŒŒì¼ ì‹œìŠ¤í…œ ì‚¬ìš©, ì„œë¸Œì—ì´ì „íŠ¸ ìœ„ì„ ê´€ë ¨ í”„ë¡¬í”„íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í†µí•©\n",
    "INSTRUCTIONS = (\n",
    "    \"# TODO MANAGEMENT\\n\"\n",
    "    + TODO_USAGE_INSTRUCTIONS\n",
    "    + \"\\n\\n\"\n",
    "    + \"=\" * 80\n",
    "    + \"\\n\\n\"\n",
    "    + \"# FILE SYSTEM USAGE\\n\"\n",
    "    + FILE_USAGE_INSTRUCTIONS\n",
    "    + \"\\n\\n\"\n",
    "    + \"=\" * 80\n",
    "    + \"\\n\\n\"\n",
    "    + \"# SUB-AGENT DELEGATION\\n\"\n",
    "    + SUBAGENT_INSTRUCTIONS\n",
    ")\n",
    "\n",
    "# í†µí•©ëœ í”„ë¡¬í”„íŠ¸ë¥¼ í™”ë©´ì— ì¶œë ¥í•˜ì—¬ ì—ì´ì „íŠ¸ì˜ ì—­í•  ë° ì‚¬ìš©ë²• ì•ˆë‚´\n",
    "show_prompt(INSTRUCTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79cd9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Agent ìƒì„± ë° ì„¤ì •, ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ìƒíƒœ ìŠ¤í‚¤ë§ˆ ì ìš©\n",
    "agent = create_agent(  # updated 1.0\n",
    "    model, all_tools, system_prompt=INSTRUCTIONS, state_schema=DeepAgentState\n",
    ")\n",
    "\n",
    "# ì—ì´ì „íŠ¸ì˜ ë‚´ë¶€ ì›Œí¬í”Œë¡œìš° êµ¬ì¡° ì‹œê°í™”, Mermaid PNGë¡œ ì¶œë ¥\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1bb918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import format_messages\n",
    "\n",
    "# ì—ì´ì „íŠ¸ì—ê²Œ MCP ê°œìš” ìš”ì²­ ë©”ì‹œì§€ ì „ë‹¬ ë° ì‘ë‹µ ê²°ê³¼ ì €ì¥\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Give me an overview of Model Context Protocol (MCP).\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# ì—ì´ì „íŠ¸ ì‘ë‹µ ë©”ì‹œì§€ í¬ë§·íŒ… ë° ì¶œë ¥, ì‚¬ìš©ì ì¹œí™”ì  ê²°ê³¼ í™•ì¸ ëª©ì \n",
    "format_messages(result[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4a0f3",
   "metadata": {},
   "source": [
    "**ì‹¤í–‰ ê²°ê³¼ ì˜ˆì‹œ ë° ì°¸ê³  ìë£Œ**\n",
    "\n",
    "ì•„ë˜ ë§í¬ì—ì„œëŠ” ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œ êµ¬í˜„í•œ Deep Agentì˜ ì‹¤ì œ ì‹¤í–‰ ê²°ê³¼ì™€ ì›Œí¬í”Œë¡œìš°ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "ì—ì´ì „íŠ¸ì˜ ë™ì‘ ë°©ì‹, ê²€ìƒ‰ ë° ìš”ì•½ ì²˜ë¦¬, íŒŒì¼ ì €ì¥ ë“± ì „ì²´ í”„ë¡œì„¸ìŠ¤ê°€ LangChain Smith í”Œë«í¼ì—ì„œ ì‹œê°ì ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤.\n",
    "\n",
    "- [ì‹¤í–‰ íŠ¸ë ˆì´ìŠ¤ ì˜ˆì‹œ ë³´ê¸°](https://smith.langchain.com/public/3a389ec6-8e6e-4f9e-9a82-0d0a9569e6f8/r)\n",
    "\n",
    "ì´ ìë£Œë¥¼ ì°¸ê³ í•˜ë©´, ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œ ë‹¤ë£¨ëŠ” Deep Agentì˜ êµ¬ì¡°ì™€ ê° ë‹¨ê³„ë³„ ë™ì‘ì„ ë”ìš± ëª…í™•í•˜ê²Œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "ì‹¤ì œ ì—ì´ì „íŠ¸ì˜ ì²˜ë¦¬ íë¦„ê³¼ ê²°ê³¼ë¬¼ì„ ì§ì ‘ í™•ì¸í•˜ë©°, ì‹¤ìŠµ ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ìƒí™©ì— ëŒ€í•œ ì´í•´ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1116545b",
   "metadata": {},
   "source": [
    "## deepagents íŒ¨í‚¤ì§€ í™œìš©í•˜ê¸°\n",
    "\n",
    "ì§€ê¸ˆê¹Œì§€ Deep Agentì˜ í•µì‹¬ íŒ¨í„´ê³¼ êµ¬ì¡°ë¥¼ ì§ì ‘ êµ¬í˜„í•˜ë©° í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤.  \n",
    "ì´ì œëŠ” ì´ëŸ¬í•œ íŒ¨í„´ì„ ê°„í¸í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì¶”ìƒí™”ëœ [`deepagents` íŒ¨í‚¤ì§€](https://github.com/hwchase17/deepagents)ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "`deepagents` íŒ¨í‚¤ì§€ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” ê¸°ëŠ¥ì„ ê¸°ë³¸ì ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "- íŒŒì¼ ì‹œìŠ¤í…œ ë„êµ¬(file system tools) ë‚´ì¥\n",
    "- TODO ê´€ë¦¬ ë„êµ¬(todo tool) í¬í•¨\n",
    "- ì‘ì—… ìœ„ì„ ë„êµ¬(task tool) ì œê³µ\n",
    "\n",
    "ì‚¬ìš©ìëŠ” ì„œë¸Œì—ì´ì „íŠ¸(sub-agent)ì™€ í•´ë‹¹ ì„œë¸Œì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•  ë„êµ¬ë§Œ ì§€ì •í•˜ë©´, ë³µì¡í•œ ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ êµ¬ì¡°ë¥¼ ì†ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ì²˜ëŸ¼ `deepagents` íŒ¨í‚¤ì§€ë¥¼ í™œìš©í•˜ë©´, ì•ì„œ ë°°ìš´ êµ¬ì¡°ì™€ ì›Œí¬í”Œë¡œìš°ë¥¼ ë”ìš± ë¹ ë¥´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "ì§ì ‘ êµ¬í˜„í•œ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ, ì‹¤ë¬´ í™˜ê²½ì—ì„œë„ ì‹ ì†í•˜ê²Œ Deep Agentë¥¼ ìƒì„±í•˜ê³  ìš´ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44016e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepagents íŒ¨í‚¤ì§€ ê¸°ë°˜ Deep Agent ìƒì„± ë° ì„¤ì •\n",
    "from deepagents import create_deep_agent\n",
    "\n",
    "# ì´¬ì˜ ì´í›„ ì½”ë“œ ì—…ë°ì´íŠ¸ ë°˜ì˜\n",
    "\n",
    "# ë¦¬ì„œì¹˜ ì„œë¸Œì—ì´ì „íŠ¸ êµ¬ì„± ì •ë³´, ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë° ë„êµ¬ í• ë‹¹\n",
    "research_sub_agent_2 = {\n",
    "    \"name\": \"research-agent\",\n",
    "    \"description\": \"Delegate research to the sub-agent researcher. Only give this researcher one topic at a time.\",\n",
    "    \"system_prompt\": RESEARCHER_INSTRUCTIONS.format(date=get_today_str()),\n",
    "    \"tools\": [tavily_search, think_tool],\n",
    "}\n",
    "\n",
    "# Deep Agent ìƒì„±, ì „ì²´ ë„êµ¬, ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ì„œë¸Œì—ì´ì „íŠ¸, ëª¨ë¸ ì „ë‹¬\n",
    "agent = create_deep_agent(  # ì—…ë°ì´íŠ¸ëœ í•¨ìˆ˜ ì‚¬ìš©\n",
    "    tools=sub_agent_tools,\n",
    "    system_prompt=INSTRUCTIONS,\n",
    "    subagents=[research_sub_agent_2],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# ì—ì´ì „íŠ¸ ë‚´ë¶€ ì›Œí¬í”Œë¡œìš° êµ¬ì¡° ì‹œê°í™” ì½”ë“œ (ì£¼ì„ ì²˜ë¦¬ë¨)\n",
    "# display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea984f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš©ì ë©”ì‹œì§€ë¡œ MCPì— ëŒ€í•œ ë§¤ìš° ê°„ë‹¨í•œ ê°œìš” ìš”ì²­\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Give me an very brief overview of Model Context Protocol (MCP).\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# ì—ì´ì „íŠ¸ ì‘ë‹µ ë©”ì‹œì§€ í¬ë§·íŒ… ë° ê²°ê³¼ ì¶œë ¥\n",
    "format_messages(result[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c617a",
   "metadata": {},
   "source": [
    "ì‹¤ì œ Deep Agentì˜ ì‹¤í–‰ ê²°ê³¼ì™€ ì›Œí¬í”Œë¡œìš°ë¥¼ í™•ì¸í•˜ê³  ì‹¶ìœ¼ì‹  ê²½ìš°, ì•„ë˜ ë§í¬ë¥¼ ì°¸ê³ í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- [ì‹¤í–‰ íŠ¸ë ˆì´ìŠ¤ ì˜ˆì‹œ ë³´ê¸°](https://smith.langchain.com/public/1d626d81-a102-4588-a2fb-cab40a7271f1/r)\n",
    "\n",
    "ì´ ìë£Œì—ì„œëŠ” ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œ êµ¬í˜„í•œ Deep Agentê°€ ì–´ë–»ê²Œ ê²€ìƒ‰, ìš”ì•½, íŒŒì¼ ì €ì¥ ë“± ê° ë‹¨ê³„ë¥¼ ì²˜ë¦¬í•˜ëŠ”ì§€ LangChain Smith í”Œë«í¼ì„ í†µí•´ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "ì‹¤ì œ ì—ì´ì „íŠ¸ì˜ ë™ì‘ íë¦„ê³¼ ê²°ê³¼ë¬¼ì„ ì§ì ‘ ì‚´í´ë³´ë©´, ë³µì¡í•œ ë¦¬ì„œì¹˜ ì›Œí¬í”Œë¡œìš°ì˜ êµ¬ì¡°ì™€ ê° ë‹¨ê³„ë³„ ì²˜ë¦¬ ê³¼ì •ì„ ë”ìš± ëª…í™•í•˜ê²Œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì‹¤ìŠµ ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ìƒí™©ì— ëŒ€í•œ ì´í•´ë„ë¥¼ ë†’ì´ê³ , Deep Agentì˜ ì „ì²´ì ì¸ êµ¬ì¡°ì™€ ì‘ë‹µ ë°©ì‹ì— ëŒ€í•œ ì‹¤ì§ˆì ì¸ ì˜ˆì‹œë¥¼ ì œê³µí•˜ë¯€ë¡œ, í•™ìŠµì— ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518bd0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Agent ìƒì„± ë° ì„¤ì •, ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ìƒíƒœ ìŠ¤í‚¤ë§ˆ ì ìš©\n",
    "agent = create_agent(  # updated 1.0\n",
    "    model, all_tools, system_prompt=INSTRUCTIONS, state_schema=DeepAgentState\n",
    ")\n",
    "\n",
    "# ì—ì´ì „íŠ¸ì˜ ë‚´ë¶€ ì›Œí¬í”Œë¡œìš° êµ¬ì¡° ì‹œê°í™”, Mermaid PNGë¡œ ì¶œë ¥\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
